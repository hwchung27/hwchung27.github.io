<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Hyung Won Chung</title>
  
  <meta name="author" content="Hyung Won Chung">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Hyung Won Chung</name>
              </p>
	      <p>I work on Machine Learning research at OpenAI.</p>
	      <p>My research focuses on Large Language Models.</p>
	      <p>Prior to that, I was at Google Brain and did my PhD at MIT.</p>
              <p style="text-align:center">
                <a href="mailto:h.w.chung27@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=1CAlXvYAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/hwchung27">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/hwchung27/">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile2.JPG"><img
		      style="width:100%;max-width:100%" alt="profile photo"
		      src="images/profile2.JPG" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Recent papers <a href="https://scholar.google.com/citations?user=1CAlXvYAAAAJ&hl=en">(see all)</a></heading>
            </td>
          </tr>
        </tbody></table>
	      
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>		
	<!-- UniMax paper-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2304.09151">
                	<papertitle>UniMax: Fairer and more Effective Language Sampling for Large-Scale Multilingual Pretraining</papertitle>
                </a>
                <br>
		<strong>Hyung Won Chung*</strong>, Noah Constant*, Xavier Garcia*, Adam Roberts, Yi Tay, Sharan Narang, Orhan Firat
                <br>
		<em>ICLR 2023</em>
                <br>
            </td>
          </tr>
	<!-- Flan collection paper-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2301.13688">
                	<papertitle>The Flan Collection: Designing Data and Methods for Effective Instruction Tuning</papertitle>
                </a>
                <br>
		Shayne Longpre, Le Hou, Tu Vu, Albert Webson, <strong>Hyung Won Chung</strong>, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, Adam Roberts
                <br>
		<em>ICML 2023</em>
                <br>
            </td>
          </tr>
	<!-- Med PaLM paper-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2212.13138">
                	<papertitle>Large Language Models Encode Clinical Knowledge</papertitle>
                </a>
                <br>
		Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, <strong>Hyung Won Chung</strong>, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Dale Webster, Greg S Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, Vivek Natarajan
                <br>
		<em>arXiv preprint arXiv:2212.13138.</em>
                <br>
            </td>
          </tr>
	<!-- BLOOM paper-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2211.05100">
                	<papertitle>BLOOM: A 176B-Parameter Open-Access Multilingual Language Model</papertitle>
                </a>
                <br>
		BigScience Workshop (more than 300 authors).
                <br>
		<em>arXiv preprint arXiv:2211.05100.</em>
                <br>
            </td>
          </tr>
	 <!-- Scaling instruction finetuning paper-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2210.11416">
                	<papertitle>Scaling Instruction-Finetuned Language Models</papertitle>
                </a>
                <br>
                <strong>Hyung Won Chung*</strong>, Le Hou*, Shayne Longpre*, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, Jason Wei*
                <br>
		<em>arXiv preprint arXiv:2210.11416.</em>
                <br>
            </td>
          </tr>
	 <!-- UL2R paper -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2210.11399">
                	<papertitle>Transcending Scaling Laws with 0.1% Extra Compute</papertitle>
                </a>
                <br>
		Yi Tay, Jason Wei, <strong>Hyung Won Chung </strong>, and 20 others.
                <br>
		<em>arXiv preprint arXiv:2210.11399.</em>
                <br>
            </td>
          </tr>
	 <!-- BBH paper -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2210.09261">
                	<papertitle>Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them</papertitle>
                </a>
                <br>
		Mirac Suzgun, Nathan Scales, Nathanael Sch√§rli, Sebastian Gehrmann, Yi Tay, <strong>Hyung Won Chung </strong>, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, Jason Wei
                <br>
		<em>arXiv preprint arXiv:2210.09261.</em>
                <br>
            </td>
          </tr>
	 <!-- BBH paper -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2210.03057">
                	<papertitle>Language Models are Multilingual Chain-of-Thought Reasoners</papertitle>
                </a>
                <br>
		Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
		Srivats, Soroush Vosoughi, <strong>Hyung Won Chung </strong>, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, Jason Wei
                <br>
		<em>arXiv preprint arXiv:2210.03057.</em>
                <br>
            </td>
          </tr>
	 <!-- scaling law vs model architecture -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2207.10551">
                	<papertitle>Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?</papertitle>
                </a>
                <br>
		Yi Tay, Mostafa Dehghani, Samira Abnar, <strong>Hyung Won Chung</strong>, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q. Tran, Dani Yogatama, Donald Metzler
                <br>
		<em>arXiv preprint arXiv:2207.10551.</em>
                <br>
            </td>
          </tr>
	 <!-- UL2 -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2205.05131">
                	<papertitle>UL2: Unifying Language Learning Paradigms</papertitle>
                </a>
                <br>
		Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason
		Wei, Xuezhi Wang, <strong>Hyung Won Chung</strong>, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil Houlsby, Donald Metzler
                <br>
		<em>arXiv preprint arXiv:2205.05131.</em>
                <br>
            </td>
          </tr>
	 <!-- Language model generalization -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2204.05832">
                	<papertitle>What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</papertitle>
                </a>
                <br>
		Thomas Wang, Adam Roberts, Daniel Hesslow, Teven Le Scao,
		<strong>Hyung Won Chung</strong>, Iz Beltagy, Julien Launay, Colin Raffel
                <br>
		<em>ICML 2022</em>
                <br>
            </td>
          </tr>
	 <!-- PaLM -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2204.02311">
                	<papertitle>PaLM: Scaling Language Modeling with Pathways</papertitle>
                </a>
                <br>
		Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
		Gaurav Mishra, Adam Roberts, Paul Barham, <br><strong>Hyung Won Chung</strong> and 67 others
                <br>
		<em>arXiv preprint arXiv:2204.02311.</em>
                <br>
            </td>
          </tr>
	 <!-- t5x -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2203.17189">
                	<papertitle>Scaling Up Models and Data with t5x and seqio</papertitle>
                </a>
                <br>
		Adam Roberts*, <strong>Hyung Won Chung*</strong>, Anselm
		Levskaya*, Gaurav Mishra*, James Bradbury*,
                <br>
		<em>arXiv preprint arXiv:2203.17189.</em>
                <br>
            </td>
          </tr>
	 <!-- scale efficiently -->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2109.10686">
                	<papertitle>Scale Efficiently: Insights from Pre-training and Fine-tuning Transformers</papertitle>
                </a>
                <br>
		Yi Tay, Mostafa Dehghani, Jinfeng Rao, William Fedus, Samira
		Abnar, <strong>Hyung Won Chung</strong>, Sharan Narang, Dani Yogatama, Ashish Vaswani, Donald Metzler
                <br>
		<em>ICLR 2022</em>
                <br>
            </td>
          </tr>
	 <!-- charformer-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2106.12672">
                	<papertitle>Charformer: Fast Character Transformers via Gradient-based Subword Tokenization</papertitle>
                </a>
                <br>
		Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, <strong>Hyung Won Chung</strong>, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, Donald Metzler
                <br>
		<em>ICLR 2022</em>
                <br>
            </td>
          </tr>
	 <!-- at5-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2102.11972">
                	<papertitle>Do Transformer Modifications Transfer Across Implementations and Applications?</papertitle>
                </a>
                <br>
		Sharan Narang, <strong>Hyung Won Chung</strong>, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan Ding, Jake Marcus, Adam Roberts, Colin Raffel
                <br>
		<em>EMNLP 2021</em>
                <br>
            </td>
          </tr>
	 <!-- ex2-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2102.01335">
                	<papertitle>Neural Data Augmentation via Example Extrapolation</papertitle>
                </a>
                <br>
		Kenton Lee*, Kelvin Guu*, Luheng He*, Tim Dozat*, <strong>Hyung Won Chung*</strong>
                <br>
		<em>arXiv preprint arXiv:2102.01335.</em>
                <br>
            </td>
          </tr>
	 <!-- rembert-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2010.12821">
                	<papertitle>Rethinking embedding coupling in pre-trained language models</papertitle>
                </a>
                <br>
		<strong>Hyung Won Chung*</strong>, Thibault F√©vry*, Henry Tsai, Melvin Johnson, Sebastian Ruder
                <br>
		<em>ICLR 2021</em>
                <br>
            </td>
          </tr>
	 <!-- clustered vocab-->
	  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
            	<a href="https://arxiv.org/abs/2010.12777">
                	<papertitle>Improving Multilingual Models with Language-Clustered Vocabularies</papertitle>
                </a>
                <br>
		<strong>Hyung Won Chung*</strong>, Dan Garrette, Kiat Chuan Tan, Jason Riesa
                <br>
		<em>EMNLP 2020</em>
                <br>
            </td>
          </tr>
      </td>
    </tr>
  </table>
</body>

</html>

